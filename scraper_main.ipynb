{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup, NavigableString\n",
    "import requests\n",
    "from time import sleep\n",
    "from random import randint\n",
    "import math\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "import pandas as pd\n",
    "from datetime import date\n",
    "from datetime import datetime\n",
    "import json\n",
    "\n",
    "import pymysql                        # for getting data from a SQL database\n",
    "from sqlalchemy import create_engine, text  # for establishing the connection and authentication\n",
    "\n",
    "\n",
    "from webscraper.scrap_modules import scraper_ds as mybib\n",
    "\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def connect_sql_database(secret):\n",
    "#     try:\n",
    "#         connection_string = 'mysql+pymysql://root:'+secret+'@localhost/bank'\n",
    "#         engine = create_engine(connection_string)\n",
    "#         data = pd.read_sql_query('SELECT * FROM final_project.linked_in_scrap', engine)\n",
    "#         return data \n",
    "#     except: \n",
    "#         print(\"Error connecting to mysql database.\")\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"keyword_list.txt\", \"r\") as file:\n",
    "    keywords = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Data Analytics,Data Analysis,Data Engineer,Data, Data Scientist'"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overview SQL Database\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>company</th>\n",
       "      <th>posting_date</th>\n",
       "      <th>job_description</th>\n",
       "      <th>seniority_level</th>\n",
       "      <th>job_function</th>\n",
       "      <th>industries</th>\n",
       "      <th>scraping_date</th>\n",
       "      <th>url</th>\n",
       "      <th>keyword</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [id, title, company, posting_date, job_description, seniority_level, job_function, industries, scraping_date, url, keyword]\n",
       "Index: []"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>company</th>\n",
       "      <th>posting_date</th>\n",
       "      <th>job_description</th>\n",
       "      <th>seniority_level</th>\n",
       "      <th>job_function</th>\n",
       "      <th>industries</th>\n",
       "      <th>scraping_date</th>\n",
       "      <th>url</th>\n",
       "      <th>keyword</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [id, title, company, posting_date, job_description, seniority_level, job_function, industries, scraping_date, url, keyword]\n",
       "Index: []"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty Dataframe\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>company</th>\n",
       "      <th>posting_date</th>\n",
       "      <th>job_description</th>\n",
       "      <th>seniority_level</th>\n",
       "      <th>job_function</th>\n",
       "      <th>industries</th>\n",
       "      <th>scraping_date</th>\n",
       "      <th>url</th>\n",
       "      <th>keyword</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [id, title, company, posting_date, job_description, seniority_level, job_function, industries, scraping_date, url, keyword]\n",
       "Index: []"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overview ID control\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "sql_data = mybib.connect_sql_database()\n",
    "scraper_df = mybib.initialize_empty_df()\n",
    "\n",
    "print(\"Overview SQL Database\")\n",
    "display(sql_data.head(5))\n",
    "display(sql_data.tail(5))\n",
    "\n",
    "print(\"Empty Dataframe\")\n",
    "display(scraper_df)\n",
    "\n",
    "print(\"Overview ID control\")\n",
    "id_control = sql_data['id'].tolist()\n",
    "id_control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords_input = ['Data%20Analyst']\n",
    "keywords = []\n",
    "\n",
    "\n",
    "# with open(f\"webscrap_data/{filename}\", 'rb') as file:\n",
    "#        scraper_df = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "soup_dict = {}\n",
    "links = {}\n",
    "for key in keywords:\n",
    "    # first_url = f\"https://www.linkedin.com/jobs/search?keywords={key}&location=Berlin%2C%20Berlin%2C%20Germany&locationId=&geoId=106967730&f_TPR=&distance=25&f_JT=F&f_E=2%2C3&f_PP=106967730&position=1&pageNum=0\"\n",
    "    key_name = key.replace('%20', \" \")\n",
    "        \n",
    "    # response = requests.get(first_url)\n",
    "    # response.status_code # 200 status code means OK!\n",
    "    # soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "    \n",
    "    # number_of_results = soup.find('span', class_=\"results-context-header__job-count\").text\n",
    "    # numb = int(number_of_results.replace(\",\", \"\").replace(\"+\", \"\"))\n",
    "    # # print(numb) # number of job post found \n",
    "    \n",
    "    # backend_call_url_list = create_backend_links(first_url, numb, key_name)\n",
    "    # with open(f'{key_name}_backend_urls.pkl', 'wb') as file:\n",
    "    #     pickle.dump(backend_call_url_list, file)\n",
    "    \n",
    "    # id_list = get_id_dict(backend_call_url_list)\n",
    "    \n",
    "    with tqdm(total=len(id_list), desc=\"Starting\") as pbar:\n",
    "        for id in id_list:\n",
    "            dynamic_text = f\"Progressing id: {id}\"\n",
    "            pbar.set_description(dynamic_text)\n",
    "            if id not in id_control:\n",
    "                try:\n",
    "                    scraper_df, id_control, small_soup = get_all_job_information(key_name, scraper_df, id, id_control)\n",
    "                    wait_time = randint(1,3000)\n",
    "                    pbar.set_description(f\"Sleep {wait_time} seconds\")\n",
    "                    sleep(wait_time/1000)\n",
    "                    \n",
    "                    soup_dict[id] = small_soup\n",
    "                except:\n",
    "                    print(f\"Error when scraping data from id: {id}, temp backups are created in temp_data.\")\n",
    "                    scraper_df_json = scraper_df.to_json('dataframe.json', orient='split', date_format='iso', indent=4)\n",
    "                    with open(f'temp_data/back_up_df.json', 'wb') as file:\n",
    "                        json.dump(scraper_df_json, file=file)\n",
    "                    with open('temp_data/small_soup_backup.json', 'w') as f:\n",
    "                        json.dump(small_soup, f)\n",
    "                    with open('temp_data/id_list_backup.pkl', \"wb\") as file:\n",
    "                        pickle.dump(links, file=file)\n",
    "            else:\n",
    "                pbar.set_description(f\"Will skip {id} because is already in the dataset.\")\n",
    "            pbar.update(1)\n",
    "    \n",
    "    current_date = datetime.now()\n",
    "    time = current_date.strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "    time\n",
    "\n",
    "    with open(f'webscrap_data/webscrap{str(time)}.pkl', 'wb') as file:\n",
    "        pickle.dump(scraper_df, file=file)\n",
    "    display(scraper_df.tail(5))\n",
    "    wait_time = randint(1,10000)\n",
    "    print(\"I will sleep for \" + str(wait_time/1000) + \" seconds.\")\n",
    "    sleep(wait_time/1000)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
