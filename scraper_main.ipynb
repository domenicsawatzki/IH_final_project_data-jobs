{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup, NavigableString\n",
    "import requests\n",
    "from time import sleep\n",
    "from random import randint\n",
    "import math\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "import pandas as pd\n",
    "from datetime import date\n",
    "from datetime import datetime\n",
    "import json\n",
    "\n",
    "import pymysql                        # for getting data from a SQL database\n",
    "from sqlalchemy import create_engine, text  # for establishing the connection and authentication\n",
    "\n",
    "\n",
    "from webscraper.scrap_modules import scraper_ds as mybib\n",
    "\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def connect_sql_database(secret):\n",
    "#     try:\n",
    "#         connection_string = 'mysql+pymysql://root:'+secret+'@localhost/bank'\n",
    "#         engine = create_engine(connection_string)\n",
    "#         data = pd.read_sql_query('SELECT * FROM final_project.linked_in_scrap', engine)\n",
    "#         return data \n",
    "#     except: \n",
    "#         print(\"Error connecting to mysql database.\")\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[autoreload of webscraper.scrap_modules.scraper_ds failed: Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Domen\\AppData\\Roaming\\Python\\Python310\\site-packages\\IPython\\extensions\\autoreload.py\", line 276, in check\n",
      "    superreload(m, reload, self.old_objects)\n",
      "  File \"C:\\Users\\Domen\\AppData\\Roaming\\Python\\Python310\\site-packages\\IPython\\extensions\\autoreload.py\", line 475, in superreload\n",
      "    module = reload(module)\n",
      "  File \"C:\\Users\\Domen\\anaconda3\\lib\\importlib\\__init__.py\", line 169, in reload\n",
      "    _bootstrap._exec(spec, module)\n",
      "  File \"<frozen importlib._bootstrap>\", line 619, in _exec\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 879, in exec_module\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 1017, in get_code\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 947, in source_to_code\n",
      "  File \"<frozen importlib._bootstrap>\", line 241, in _call_with_frames_removed\n",
      "  File \"c:\\Users\\Domen\\IronHack\\01_projects\\IH_final_project_data-jobs\\webscraper\\scrap_modules\\scraper_ds.py\", line 17\n",
      "    keywords = pd.read_csv(file)\n",
      "    ^^^^^^^^\n",
      "IndentationError: expected an indented block after 'with' statement on line 16\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "# with open(\"keyword_list.csv\", \"r\") as file:\n",
    "#     keywords = pd.read_csv(file, delimiter= \";\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keywords[\"Keywords\"] = keywords[\"Keywords\"].str.replace(\" \", \"%20\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Data%20Analyst', 'Data%20Analysis', 'Data%20Scientist', 'Data%20']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# keywords_list = keywords['Keywords'].tolist()\n",
    "keywords_list = mybib.import_keyword_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overview SQL Database\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>company</th>\n",
       "      <th>posting_date</th>\n",
       "      <th>job_description</th>\n",
       "      <th>seniority_level</th>\n",
       "      <th>job_function</th>\n",
       "      <th>industries</th>\n",
       "      <th>scraping_date</th>\n",
       "      <th>url</th>\n",
       "      <th>keyword</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [id, title, company, posting_date, job_description, seniority_level, job_function, industries, scraping_date, url, keyword]\n",
       "Index: []"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>company</th>\n",
       "      <th>posting_date</th>\n",
       "      <th>job_description</th>\n",
       "      <th>seniority_level</th>\n",
       "      <th>job_function</th>\n",
       "      <th>industries</th>\n",
       "      <th>scraping_date</th>\n",
       "      <th>url</th>\n",
       "      <th>keyword</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [id, title, company, posting_date, job_description, seniority_level, job_function, industries, scraping_date, url, keyword]\n",
       "Index: []"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty Dataframe\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>company</th>\n",
       "      <th>posting_date</th>\n",
       "      <th>job_description</th>\n",
       "      <th>seniority_level</th>\n",
       "      <th>job_function</th>\n",
       "      <th>industries</th>\n",
       "      <th>scraping_date</th>\n",
       "      <th>url</th>\n",
       "      <th>keyword</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [id, title, company, posting_date, job_description, seniority_level, job_function, industries, scraping_date, url, keyword]\n",
       "Index: []"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overview ID control\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "sql_data = mybib.connect_sql_database()\n",
    "scraper_df = mybib.initialize_empty_df()\n",
    "\n",
    "print(\"Overview SQL Database\")\n",
    "display(sql_data.head(5))\n",
    "display(sql_data.tail(5))\n",
    "\n",
    "print(\"Empty Dataframe\")\n",
    "display(scraper_df)\n",
    "\n",
    "print(\"Overview ID control\")\n",
    "id_control = sql_data['id'].tolist()\n",
    "id_control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords_input = ['Data%20Analyst']\n",
    "keywords = []\n",
    "\n",
    "\n",
    "# with open(f\"webscrap_data/{filename}\", 'rb') as file:\n",
    "#        scraper_df = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "soup_dict = {}\n",
    "links = {}\n",
    "for key in keywords:\n",
    "    # first_url = f\"https://www.linkedin.com/jobs/search?keywords={key}&location=Berlin%2C%20Berlin%2C%20Germany&locationId=&geoId=106967730&f_TPR=&distance=25&f_JT=F&f_E=2%2C3&f_PP=106967730&position=1&pageNum=0\"\n",
    "    key_name = key.replace('%20', \" \")\n",
    "        \n",
    "    # response = requests.get(first_url)\n",
    "    # response.status_code # 200 status code means OK!\n",
    "    # soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "    \n",
    "    # number_of_results = soup.find('span', class_=\"results-context-header__job-count\").text\n",
    "    # numb = int(number_of_results.replace(\",\", \"\").replace(\"+\", \"\"))\n",
    "    # # print(numb) # number of job post found \n",
    "    \n",
    "    # backend_call_url_list = create_backend_links(first_url, numb, key_name)\n",
    "    # with open(f'{key_name}_backend_urls.pkl', 'wb') as file:\n",
    "    #     pickle.dump(backend_call_url_list, file)\n",
    "    \n",
    "    # id_list = get_id_dict(backend_call_url_list)\n",
    "    \n",
    "    with tqdm(total=len(id_list), desc=\"Starting\") as pbar:\n",
    "        for id in id_list:\n",
    "            dynamic_text = f\"Progressing id: {id}\"\n",
    "            pbar.set_description(dynamic_text)\n",
    "            if id not in id_control:\n",
    "                try:\n",
    "                    scraper_df, id_control, small_soup = get_all_job_information(key_name, scraper_df, id, id_control)\n",
    "                    wait_time = randint(1,3000)\n",
    "                    pbar.set_description(f\"Sleep {wait_time} seconds\")\n",
    "                    sleep(wait_time/1000)\n",
    "                    \n",
    "                    soup_dict[id] = small_soup\n",
    "                except:\n",
    "                    print(f\"Error when scraping data from id: {id}, temp backups are created in temp_data.\")\n",
    "                    scraper_df_json = scraper_df.to_json('dataframe.json', orient='split', date_format='iso', indent=4)\n",
    "                    with open(f'temp_data/back_up_df.json', 'wb') as file:\n",
    "                        json.dump(scraper_df_json, file=file)\n",
    "                    with open('temp_data/small_soup_backup.json', 'w') as f:\n",
    "                        json.dump(small_soup, f)\n",
    "                    with open('temp_data/id_list_backup.pkl', \"wb\") as file:\n",
    "                        pickle.dump(links, file=file)\n",
    "            else:\n",
    "                pbar.set_description(f\"Will skip {id} because is already in the dataset.\")\n",
    "            pbar.update(1)\n",
    "    \n",
    "    current_date = datetime.now()\n",
    "    time = current_date.strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "    time\n",
    "\n",
    "    with open(f'webscrap_data/webscrap{str(time)}.pkl', 'wb') as file:\n",
    "        pickle.dump(scraper_df, file=file)\n",
    "    display(scraper_df.tail(5))\n",
    "    wait_time = randint(1,10000)\n",
    "    print(\"I will sleep for \" + str(wait_time/1000) + \" seconds.\")\n",
    "    sleep(wait_time/1000)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
